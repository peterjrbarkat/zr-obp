================================================
Evaluation of OPE
================================================
Here, we describe an experimental protocol to evaluate OPE estimators.

We can empirically evaluate OPE estimators' performances by using two sources of logged bandit feedback collected by running two different policies.
We regard one policy as behavior policy :math:`\pi_b` and the other one as evaluation policy :math:`\pi_e`.
We denote log data generated by :math:`\pi_e` and :math:`\pi_b` as :math:`\calD^{(e)} := \{ (x^{(e)}_t, a^{(e)}_t, r^{(e)}_t) \}_{t=1}^T` and :math:`\calD^{(b)} := \{ (x^{(b)}_t, a^{(b)}_t, r^{(b)}_t) \}_{t=1}^T`, respectively.
Then, by applying the following protocol to several different OPE estimators, we can compare their estimation performances:


1. Define the evaluation and test sets as:

    * in-sample case: :math:`\calD_{\mathrm{ev}} := \calD^{(b)}_{1:T}`, :math:`\calD_{\mathrm{te}} := \calD^{(e)}_{1:T}`
    * out-sample case: :math:`\calD_{\mathrm{ev}} := \calD^{(b)}_{1:\tilde{t}}`, :math:`\calD_{\mathrm{te}} := \calD^{(e)}_{\tilde{t}+1:T}`

    where :math:`\calD_{a:b} := \{ (x_t,a_t,r_t) \}_{t=a}^{b}`.

2. Estimate the policy value of :math:`\pi_e` using :math:`\calD_{\mathrm{ev}}` by an estimator :math:`\hat{V}`. We can represent a policy value estimated by :math:`\hat{V}` as :math:`\hat{V} (\pi_e; \calD_{\mathrm{ev}})`.

3. Estimate :math:`V(\pi_e)` by the *on-policy estimation* as follows and regard it as the ground-truth.

    .. math::
        V_{\mathrm{on}} (\pi_e; \calD_{\mathrm{te}}) := \E_{\calD_{\mathrm{te}}} [r^{(e)}_t].

4. Compare the off-policy estimate :math:`\hat{V}(\pi_e; \calD_{\mathrm{ev}})` with its ground-truth :math:`V_{\mathrm{on}} (\pi_e; \calD_{\mathrm{te}})`. We can evaluate the estimation accuracy of :math:`\hat{V}` by the following *relative estimation error* (relative-EE):

    .. math::
        \textit{relative-EE} (\hat{V}; \calD_{\mathrm{ev}}) := \left| \frac{\hat{V} (\pi_e; \calD_{\mathrm{ev}}) - V_{\mathrm{on}} (\pi_e; \calD_{\mathrm{te}}) }{V_{\mathrm{on}} (\pi_e; \calD_{\mathrm{te}})} \right|.

5. To estimate standard deviation of relative-EE, repeat the above process several times with different bootstrap samples of the logged bandit data created by sampling data *with replacement* from :math:`\calD_{\mathrm{ev}}`.

We call the problem setting **without** the sample splitting by time series as in-sample case.
In contrast, we call that **with** the sample splitting as out-sample case, where OPE estimators aim to estimate the policy value of an evaluation policy in the future data.
The standard OPE assumes the in-sample case where there are no distributional change in the environment over time.
However, in practice, we aim to estimate the performance of an evaluation policy in the future, which may introduce the distributional change between the data used to conduct OPE and the environment that defines the policy value of the evaluation policy.
It is thus valuable to test the performance of OPE estimators in the out-sample case in addition to the in-sample case.

The following algorithm describes the detailed experimental protocol to evaluate OPE estimators.

.. image:: ./_static/images/evaluation_of_ope_algo.png
   :scale: 28%
   :align: center


Using the above protocol, our real-world data, and pipeline, we have performed extensive benchmark experiments on a variety of existing off-policy estimators.
The experimental results and the relevant discussion can be found in `our paper <https://arxiv.org/abs/2008.07146>`_.
The code for running the benchmark experiments can be found at `zr-obp/benchmark/ope <https://github.com/st-tech/zr-obp/tree/master/benchmark>`_.
